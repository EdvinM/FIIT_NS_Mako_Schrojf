{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'batch_size': 16, 'num_hidden_layers': 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = yaml.safe_load(open('../src/models/hparams.yaml'))\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>img_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17/10000217_1981-05-05_2009.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>[255 255 255 ... 144  78  27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/100012_1948-07-03_2008.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>[92 98 93 ... 35 31 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/10002116_1971-05-31_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>[ 10  30  61 ... 231 237 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/10002702_1960-11-09_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>[178 122  97 ... 168 112  83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41/10003541_1937-09-27_1971.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "      <td>[194 189 190 ... 101 103 104]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         full_path  gender  age                      img_array\n",
       "0  17/10000217_1981-05-05_2009.jpg     1.0   28  [255 255 255 ... 144  78  27]\n",
       "1    12/100012_1948-07-03_2008.jpg     1.0   60        [92 98 93 ... 35 31 30]\n",
       "2  16/10002116_1971-05-31_2012.jpg     0.0   41  [ 10  30  61 ... 231 237 255]\n",
       "3  02/10002702_1960-11-09_2012.jpg     0.0   52  [178 122  97 ... 168 112  83]\n",
       "4  41/10003541_1937-09-27_1971.jpg     1.0   34  [194 189 190 ... 101 103 104]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.read_csv('../data/processed/wiki_df.csv', sep=';')\n",
    "\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noramalize image values on range <0.0;1.0>\n",
    "# check: https://www.tensorflow.org/tutorials/images/cnn\n",
    "#wiki_df['img_array'] = wiki_df['img_array'] / 255.0\n",
    "#wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "wiki_df['img_array'] = wiki_df['full_path'].apply(lambda x: np.array(cv2.resize(cv2.imread('../data/raw/wiki_crop/' + x), (224, 224), interpolation=cv2.INTER_LINEAR).reshape(1, -1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "* Ako prvé sa pokúsime vytvoriť NN podobnú VGG. Podľa [WEEK_7 lab](https://github.com/matus-pikuliak/neural_networks_at_fiit/blob/92b24eef8e6444c43a22e8fa51a349b3b1043a7c/week_7/week_7.ipynb), alebo iného tutoriálu\n",
    "* Natrénujeme ju na už predspracovanom datasete\n",
    "* Jej výsledok pou%zijeme ako štartovaciu čiaru\n",
    "* Túto sieť budeme rozširovať o ďalšie vrstvy a parametre\n",
    "* V projekte ponecháme sieť s najlepším skóre\n",
    "* Dole pripájam referenčnú ukážku siete. [Zdroj](https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/?__s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dropout\n",
    "\n",
    "\n",
    "class MiniVGGNetModel(keras.Model):\n",
    "    def __init__(self, classes, chanDim=-1):\n",
    "        # call the parent constructor\n",
    "        super(MiniVGGNetModel, self).__init__()\n",
    "\n",
    "        # initialize the layers in the first (CONV => RELU) * 2 => POOL\n",
    "        # layer set\n",
    "        self.conv1A = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act1A = Activation(\"relu\")\n",
    "        self.bn1A = BatchNormalization(axis=chanDim)\n",
    "        self.conv1B = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act1B = Activation(\"relu\")\n",
    "        self.bn1B = BatchNormalization(axis=chanDim)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        # initialize the layers in the second (CONV => RELU) * 2 => POOL\n",
    "        # layer set\n",
    "        self.conv2A = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act2A = Activation(\"relu\")\n",
    "        self.bn2A = BatchNormalization(axis=chanDim)\n",
    "        self.conv2B = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act2B = Activation(\"relu\")\n",
    "        self.bn2B = BatchNormalization(axis=chanDim)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        # initialize the layers in our fully-connected layer set\n",
    "        self.flatten = Flatten()\n",
    "        self.dense3 = Dense(512)\n",
    "        self.act3 = Activation(\"relu\")\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.do3 = Dropout(0.5)\n",
    "\n",
    "        # initialize the layers in the softmax classifier layer set\n",
    "        self.dense4 = Dense(classes)\n",
    "        self.softmax = Activation(\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # build the first (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv1A(inputs)\n",
    "        x = self.act1A(x)\n",
    "        x = self.bn1A(x)\n",
    "        x = self.conv1B(x)\n",
    "        x = self.act1B(x)\n",
    "        x = self.bn1B(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # build the second (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv2A(x)\n",
    "        x = self.act2A(x)\n",
    "        x = self.bn2A(x)\n",
    "        x = self.conv2B(x)\n",
    "        x = self.act2B(x)\n",
    "        x = self.bn2B(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # build our FC layer set\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.do3(x)\n",
    "\n",
    "        # build the softmax classifier\n",
    "        x = self.dense4(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        # return the constructed model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wiki_df['age'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (22578, 150528) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2799d636c46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (22578, 150528) and data type uint8"
     ]
    }
   ],
   "source": [
    "classes = 101 #0 to 100\n",
    "target = wiki_df['age'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)\n",
    "\n",
    "features = [] \n",
    "for i in range(0, len(wiki_df['img_array'].values)):\n",
    "    features.append(wiki_df['img_array'].values[i])\n",
    " \n",
    "features = np.array(features)\n",
    "features = features.reshape(features.shape[0], 224, 224, 3)\n",
    "\n",
    "#features = wiki_df['img_array'].values\n",
    "#features = features / 255.0\n",
    "#features = features.reshape(features.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22578\n",
      "22578\n",
      "22578\n",
      "(22578, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(wiki_df['img_array'].values))\n",
    "print(len(target_classes))\n",
    "print(len(features))\n",
    "print((features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(22578/2)\n",
    "\n",
    "train_images = features[0:split]\n",
    "#train_images = train_images / 255.0\n",
    "train_labels = target_classes[0:split]\n",
    "\n",
    "test_images = features[split:]\n",
    "#test_images = test_images / 255.0\n",
    "test_labels = target_classes[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (3763, 224, 224, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-76edddc16767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mHACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (3763, 224, 224, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "HACK = int(22578/6)\n",
    "\n",
    "for i in range(6):\n",
    "    start = i*HACK\n",
    "    end = (i+1)*HACK\n",
    "    \n",
    "    train_images[start:end] = train_images[start:end] / 255.0\n",
    "    test_images[start:end] = test_images[start:end] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(test_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-37-7fe3c9ba7f72>:43 call  *\n        x = self.conv1A(inputs)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:197 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:1134 __call__\n        return self.conv_op(inp, filter)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:639 __call__\n        return self.call(inp, filter)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:238 __call__\n        name=self.name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:2010 conv2d\n        name=name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py:1071 conv2d\n        data_format=data_format, dilations=dilations, name=name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:631 _apply_op_helper\n        param_name=input_name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:60 _SatisfiesTypeConstraint\n        \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\n    TypeError: Value passed to parameter 'input' has DataType uint8 not in list of allowed values: float16, bfloat16, float32, float64\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-59e45be4a945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     epochs = epochs)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2620\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2707\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-37-7fe3c9ba7f72>:43 call  *\n        x = self.conv1A(inputs)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:197 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:1134 __call__\n        return self.conv_op(inp, filter)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:639 __call__\n        return self.call(inp, filter)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:238 __call__\n        name=self.name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:2010 conv2d\n        name=name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py:1071 conv2d\n        data_format=data_format, dilations=dilations, name=name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:631 _apply_op_helper\n        param_name=input_name)\n    /home/viliam/.virtualenvs/neural_networks_at_fiit/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:60 _SatisfiesTypeConstraint\n        \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\n    TypeError: Value passed to parameter 'input' has DataType uint8 not in list of allowed values: float16, bfloat16, float32, float64\n"
     ]
    }
   ],
   "source": [
    "model = MiniVGGNetModel(\n",
    "    classes = classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.TensorBoard(\n",
    "    #    log_dir=os.path.join(\"logs\", timestamp()),\n",
    "    #    histogram_freq=1,\n",
    "    #    profile_batch=0)\n",
    "]\n",
    "\n",
    "# callbacks = []  # If you do not want to log results into TensorBoard\n",
    "\n",
    "scores = []\n",
    "epochs = 60\n",
    "batch_size = 64\n",
    "\n",
    "score = model.fit(\n",
    "    x=train_images,\n",
    "    y=train_labels,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (test_images, test_labels),\n",
    "    callbacks = callbacks,\n",
    "    epochs = epochs)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "model.summary()  # Writes number of parameters for each layer at the end of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Cely VGG model.\n",
    "\n",
    "Zdroj: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(keras.layers.Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.ZeroPadding2D((1,1)))\n",
    "model.add(keras.layers.Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(keras.layers.Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Convolution2D(2622, (1, 1)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../data/vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = keras.Model(inputs=model.layers[0].input, outputs=model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image as array using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 101 #0 to 100\n",
    "target = wiki_df['age'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)\n",
    "\n",
    "features = []\n",
    " \n",
    "for i in range(0, 1):\n",
    "    features.append(wiki_df['img_array'].values[i])\n",
    " \n",
    "features = np.array(features)\n",
    "features = features.reshape(features.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_output = keras.models.Sequential()\n",
    "base_model_output = keras.layers.Convolution2D(101, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "base_model_output = keras.layers.Flatten()(base_model_output)\n",
    "base_model_output = keras.layers.Activation('softmax')(base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model = keras.Model(inputs=model.input, outputs=base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n",
      "epoch  6\n",
      "epoch  7\n",
      "epoch  8\n",
      "epoch  9\n",
      "epoch  10\n",
      "epoch  11\n",
      "epoch  12\n",
      "epoch  13\n",
      "epoch  14\n",
      "epoch  15\n",
      "epoch  16\n",
      "epoch  17\n",
      "epoch  18\n",
      "epoch  19\n",
      "epoch  20\n",
      "epoch  21\n",
      "epoch  22\n",
      "epoch  23\n",
      "epoch  24\n",
      "epoch  25\n",
      "epoch  26\n",
      "epoch  27\n",
      "epoch  28\n",
      "epoch  29\n",
      "epoch  30\n",
      "epoch  31\n",
      "epoch  32\n",
      "epoch  33\n",
      "epoch  34\n",
      "epoch  35\n",
      "epoch  36\n",
      "epoch  37\n",
      "epoch  38\n",
      "epoch  39\n",
      "epoch  40\n",
      "epoch  41\n",
      "epoch  42\n",
      "epoch  43\n",
      "epoch  44\n",
      "epoch  45\n",
      "epoch  46\n",
      "epoch  47\n",
      "epoch  48\n",
      "epoch  49\n",
      "epoch  50\n",
      "epoch  51\n",
      "epoch  52\n",
      "epoch  53\n",
      "epoch  54\n",
      "epoch  55\n",
      "epoch  56\n",
      "epoch  57\n",
      "epoch  58\n",
      "epoch  59\n",
      "epoch  60\n",
      "epoch  61\n",
      "epoch  62\n",
      "epoch  63\n",
      "epoch  64\n",
      "epoch  65\n",
      "epoch  66\n",
      "epoch  67\n",
      "epoch  68\n",
      "epoch  69\n",
      "epoch  70\n",
      "epoch  71\n",
      "epoch  72\n",
      "epoch  73\n",
      "epoch  74\n",
      "epoch  75\n",
      "epoch  76\n",
      "epoch  77\n",
      "epoch  78\n",
      "epoch  79\n",
      "epoch  80\n",
      "epoch  81\n",
      "epoch  82\n",
      "epoch  83\n",
      "epoch  84\n",
      "epoch  85\n",
      "epoch  86\n",
      "epoch  87\n",
      "epoch  88\n",
      "epoch  89\n",
      "epoch  90\n",
      "epoch  91\n",
      "epoch  92\n",
      "epoch  93\n",
      "epoch  94\n",
      "epoch  95\n",
      "epoch  96\n",
      "epoch  97\n",
      "epoch  98\n",
      "epoch  99\n",
      "epoch  100\n",
      "epoch  101\n",
      "epoch  102\n",
      "epoch  103\n",
      "epoch  104\n",
      "epoch  105\n",
      "epoch  106\n",
      "epoch  107\n",
      "epoch  108\n",
      "epoch  109\n",
      "epoch  110\n",
      "epoch  111\n",
      "epoch  112\n",
      "epoch  113\n",
      "epoch  114\n",
      "epoch  115\n",
      "epoch  116\n",
      "epoch  117\n",
      "epoch  118\n",
      "epoch  119\n",
      "epoch  120\n",
      "epoch  121\n",
      "epoch  122\n",
      "epoch  123\n",
      "epoch  124\n",
      "epoch  125\n",
      "epoch  126\n",
      "epoch  127\n",
      "epoch  128\n",
      "epoch  129\n",
      "epoch  130\n",
      "epoch  131\n",
      "epoch  132\n",
      "epoch  133\n",
      "epoch  134\n",
      "epoch  135\n",
      "epoch  136\n",
      "epoch  137\n",
      "epoch  138\n",
      "epoch  139\n",
      "epoch  140\n",
      "epoch  141\n",
      "epoch  142\n",
      "epoch  143\n",
      "epoch  144\n",
      "epoch  145\n",
      "epoch  146\n",
      "epoch  147\n",
      "epoch  148\n",
      "epoch  149\n",
      "epoch  150\n",
      "epoch  151\n",
      "epoch  152\n",
      "epoch  153\n",
      "epoch  154\n",
      "epoch  155\n",
      "epoch  156\n",
      "epoch  157\n",
      "epoch  158\n",
      "epoch  159\n",
      "epoch  160\n",
      "epoch  161\n",
      "epoch  162\n",
      "epoch  163\n",
      "epoch  164\n",
      "epoch  165\n",
      "epoch  166\n",
      "epoch  167\n",
      "epoch  168\n",
      "epoch  169\n",
      "epoch  170\n",
      "epoch  171\n",
      "epoch  172\n",
      "epoch  173\n",
      "epoch  174\n",
      "epoch  175\n",
      "epoch  176\n",
      "epoch  177\n",
      "epoch  178\n",
      "epoch  179\n",
      "epoch  180\n",
      "epoch  181\n",
      "epoch  182\n",
      "epoch  183\n",
      "epoch  184\n",
      "epoch  185\n",
      "epoch  186\n",
      "epoch  187\n",
      "epoch  188\n",
      "epoch  189\n",
      "epoch  190\n",
      "epoch  191\n",
      "epoch  192\n",
      "epoch  193\n",
      "epoch  194\n",
      "epoch  195\n",
      "epoch  196\n",
      "epoch  197\n",
      "epoch  198\n",
      "epoch  199\n",
      "epoch  200\n",
      "epoch  201\n",
      "epoch  202\n",
      "epoch  203\n",
      "epoch  204\n",
      "epoch  205\n",
      "epoch  206\n",
      "epoch  207\n",
      "epoch  208\n",
      "epoch  209\n",
      "epoch  210\n",
      "epoch  211\n",
      "epoch  212\n",
      "epoch  213\n",
      "epoch  214\n",
      "epoch  215\n",
      "epoch  216\n",
      "epoch  217\n",
      "epoch  218\n",
      "epoch  219\n",
      "epoch  220\n",
      "epoch  221\n",
      "epoch  222\n",
      "epoch  223\n",
      "epoch  224\n",
      "epoch  225\n",
      "epoch  226\n",
      "epoch  227\n",
      "epoch  228\n",
      "epoch  229\n",
      "epoch  230\n",
      "epoch  231\n",
      "epoch  232\n",
      "epoch  233\n",
      "epoch  234\n",
      "epoch  235\n",
      "epoch  236\n",
      "epoch  237\n",
      "epoch  238\n",
      "epoch  239\n",
      "epoch  240\n",
      "epoch  241\n",
      "epoch  242\n",
      "epoch  243\n",
      "epoch  244\n",
      "epoch  245\n",
      "epoch  246\n",
      "epoch  247\n",
      "epoch  248\n",
      "epoch  249\n",
      "Train on 256 samples, validate on 6774 samples\n",
      "160/256 [=================>............] - ETA: 41s - loss: 10.1584 - accuracy: 0.0234"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6c0bf4a2b406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mix_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "age_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    " \n",
    "scores = []\n",
    "epochs = 250; batch_size = 256\n",
    " \n",
    "for i in range(epochs):\n",
    "    print(\"epoch \",i)\n",
    " \n",
    "ix_train = np.random.choice(train_x.shape[0], size=batch_size)\n",
    " \n",
    "score = age_model.fit(train_x[ix_train], train_y[ix_train], epochs=1, validation_data=(test_x, test_y))\n",
    " \n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
